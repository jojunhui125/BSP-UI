/**
 * ì¸ë±ìŠ¤ ë§¤ë‹ˆì €
 * - ì¦ë¶„ ì¸ë±ì‹± (mtime ê¸°ë°˜ ë³€ê²½ ê°ì§€)
 * - ë©€í‹° í”„ë¡œì„¸ì‹± ì§€ì›
 * - ì§„í–‰ë¥  ë¦¬í¬íŒ…
 */

import { indexDb, FileRecord, SymbolRecord, IncludeRecord, DtNodeRecord, DtPropertyRecord, GpioPinRecord } from '../database/IndexDatabase'
import { fileContentCache, symbolCache, clearAllCaches } from '../cache/LRUCache'
import { sshManager } from '../ssh/SshManager'
import { BrowserWindow } from 'electron'

// ì¸ë±ì‹± ì§„í–‰ ìƒíƒœ
export interface IndexProgress {
  phase: 'init' | 'files' | 'symbols' | 'includes' | 'dt' | 'gpio' | 'done' | 'error'
  current: number
  total: number
  message: string
  speed?: number  // files/sec
}

// íŒŒì¼ ë³€ê²½ ì •ë³´
interface FileChange {
  path: string
  name: string
  type: 'added' | 'modified' | 'deleted'
  mtime: number
}

export class IndexManager {
  private projectPath: string = ''
  private serverId: string = ''
  private isIndexing: boolean = false
  private shouldCancel: boolean = false
  private mainWindow: BrowserWindow | null = null
  private startTime: number = 0

  /**
   * ë©”ì¸ ìœˆë„ìš° ì„¤ì • (ì§„í–‰ë¥  ì „ì†¡ìš©)
   */
  setMainWindow(window: BrowserWindow): void {
    this.mainWindow = window
  }

  /**
   * ì§„í–‰ë¥  ì „ì†¡
   */
  private sendProgress(progress: IndexProgress): void {
    if (this.mainWindow) {
      this.mainWindow.webContents.send('index:progress', progress)
    } else {
      console.warn('[IndexManager] mainWindow is null, cannot send progress')
    }
  }

  /**
   * ì¸ë±ì‹± ì‹œì‘ (ì¦ë¶„)
   */
  async startIndexing(projectPath: string, serverId: string, fullReindex: boolean = false): Promise<boolean> {
    if (this.isIndexing) {
      console.log('[IndexManager] Already indexing')
      return false
    }

    this.isIndexing = true
    this.shouldCancel = false
    this.projectPath = projectPath
    this.serverId = serverId
    this.startTime = Date.now()

    try {
      // DB ì´ˆê¸°í™”
      indexDb.init(projectPath)
      
      if (fullReindex) {
        indexDb.clearAll()
        clearAllCaches()
      }

      this.sendProgress({ phase: 'init', current: 0, total: 0, message: 'ë³€ê²½ ì‚¬í•­ í™•ì¸ ì¤‘...' })

      // 1. íŒŒì¼ ë³€ê²½ ê°ì§€
      const changes = await this.detectChanges()
      
      if (changes.length === 0 && !fullReindex) {
        this.sendProgress({ phase: 'done', current: 0, total: 0, message: 'ë³€ê²½ ì‚¬í•­ ì—†ìŒ' })
        this.isIndexing = false
        return true
      }

      console.log(`[IndexManager] ${changes.length} files to process`)

      // 2. ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬
      const deleted = changes.filter(c => c.type === 'deleted')
      for (const file of deleted) {
        indexDb.deleteFile(file.path)
      }

      // 3. ì¶”ê°€/ìˆ˜ì •ëœ íŒŒì¼ ì²˜ë¦¬
      const toProcess = changes.filter(c => c.type !== 'deleted')
      
      if (toProcess.length > 0) {
        await this.processFiles(toProcess)
      }

      // 4. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
      indexDb.setMetadata('last_index_time', Date.now().toString())
      indexDb.setMetadata('project_path', projectPath)

      const stats = indexDb.getStats()
      const elapsed = Date.now() - this.startTime
      
      this.sendProgress({
        phase: 'done',
        current: toProcess.length,
        total: toProcess.length,
        message: `ì™„ë£Œ! ${stats.files}íŒŒì¼, ${stats.symbols}ì‹¬ë³¼ (${elapsed}ms)`,
        speed: toProcess.length / (elapsed / 1000)
      })

      console.log(`[IndexManager] Completed in ${elapsed}ms:`, stats)
      
      this.isIndexing = false
      return true

    } catch (err: any) {
      console.error('[IndexManager] Error:', err)
      this.sendProgress({ phase: 'error', current: 0, total: 0, message: err.message })
      this.isIndexing = false
      return false
    }
  }

  /**
   * ì¸ë±ì‹± ì·¨ì†Œ
   */
  cancelIndexing(): void {
    this.shouldCancel = true
  }

  /**
   * íŒŒì¼ ë³€ê²½ ê°ì§€ (ì¦ë¶„ ì¸ë±ì‹±ì˜ í•µì‹¬)
   */
  private async detectChanges(): Promise<FileChange[]> {
    const changes: FileChange[] = []
    
    console.log(`[IndexManager] Scanning files in: ${this.projectPath}`)
    
    // ì„œë²„ì—ì„œ í˜„ì¬ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    // 1. sources í´ë”: ëª¨ë“  íƒ€ì… í¬í•¨
    // 2. build í´ë”: DTS/DTSI íŒŒì¼ë§Œ í¬í•¨ (ì˜¤ë²„ë¼ì´ë“œëœ DT íŒŒì¼ ê²€ìƒ‰ìš©)
    const findCmd = `cd "${this.projectPath}" && { find ./sources -type f \\( -name "*.bb" -o -name "*.bbappend" -o -name "*.conf" -o -name "*.inc" -o -name "*.h" -o -name "*.dts" -o -name "*.dtsi" \\) ! -path "*/.git/*" 2>/dev/null; find . -path "*/build_*/*" -type f \\( -name "*.dts" -o -name "*.dtsi" \\) ! -path "*/tmp/work/*" 2>/dev/null; } | head -10000`
    
    console.log(`[IndexManager] Find command: ${findCmd}`)
    
    const result = await sshManager.exec(this.serverId, findCmd)

    console.log(`[IndexManager] Find result: code=${result.code}, stdout length=${result.stdout.length}, stderr=${result.stderr.slice(0, 200)}`)

    if (result.code !== 0 && result.stdout.length === 0) {
      throw new Error(`Failed to scan files: ${result.stderr}`)
    }

    const files = result.stdout.split('\n').filter(line => line.trim())
    console.log(`[IndexManager] Found ${files.length} files to check`)

    if (files.length === 0) {
      console.log('[IndexManager] No files found. Check if the path is correct.')
      return changes
    }

    // DBì˜ ê¸°ì¡´ íŒŒì¼ ëª©ë¡
    const dbFiles = indexDb.getAllFilesMtime()
    const currentFiles = new Set<string>()

    // ê° íŒŒì¼ì— ëŒ€í•´ mtime ê°€ì ¸ì˜¤ê¸° (ë°°ì¹˜ë¡œ ì²˜ë¦¬)
    const STAT_BATCH = 100
    for (let i = 0; i < files.length; i += STAT_BATCH) {
      const batch = files.slice(i, i + STAT_BATCH)
      const statCmd = batch.map(f => `stat -c '%n\t%Y' "${f}" 2>/dev/null || echo "${f}\t0"`).join('; ')
      
      try {
        const statResult = await sshManager.exec(
          this.serverId,
          `cd "${this.projectPath}" && (${statCmd})`
        )
        
        for (const line of statResult.stdout.split('\n')) {
          if (!line.trim()) continue
          
          const [relativePath, mtimeStr] = line.split('\t')
          if (!relativePath) continue
          
          const fullPath = `${this.projectPath}/${relativePath.replace(/^\.\//, '')}`
          const mtime = parseFloat(mtimeStr) || Date.now() / 1000
          const name = relativePath.split('/').pop() || ''
          
          currentFiles.add(fullPath)
          
          const dbMtime = dbFiles.get(fullPath)
          
          if (dbMtime === undefined) {
            // ìƒˆ íŒŒì¼
            changes.push({ path: fullPath, name, type: 'added', mtime })
          } else if (mtime > dbMtime) {
            // ìˆ˜ì •ëœ íŒŒì¼
            changes.push({ path: fullPath, name, type: 'modified', mtime })
          }
        }
      } catch (err) {
        console.warn(`[IndexManager] Failed to stat batch ${i}-${i + STAT_BATCH}:`, err)
        // ì—ëŸ¬ê°€ ë‚˜ë„ ê³„ì† ì§„í–‰ (í•´ë‹¹ ë°°ì¹˜ë§Œ ìŠ¤í‚µ)
      }
      
      // ì§„í–‰ë¥  í‘œì‹œ
      if (i % 500 === 0 && i > 0) {
        console.log(`[IndexManager] Scanned ${i}/${files.length} files`)
      }
    }

    // ì‚­ì œëœ íŒŒì¼ ê°ì§€
    for (const [path] of dbFiles) {
      if (!currentFiles.has(path)) {
        changes.push({ path, name: path.split('/').pop() || '', type: 'deleted', mtime: 0 })
      }
    }

    console.log(`[IndexManager] Changes detected: ${changes.length} (added: ${changes.filter(c => c.type === 'added').length}, modified: ${changes.filter(c => c.type === 'modified').length}, deleted: ${changes.filter(c => c.type === 'deleted').length})`)

    return changes
  }

  /**
   * íŒŒì¼ ì²˜ë¦¬ (ì•ˆì •ì  ë³‘ë ¬ ì²˜ë¦¬)
   * SSH ë™ì‹œ ìš”ì²­ 8ê°œê¹Œì§€ í—ˆìš©
   */
  private async processFiles(files: FileChange[]): Promise<void> {
    const BATCH_SIZE = 6  // ì•ˆì •ì„±: 12â†’6 (SSH ë™ì‹œ 8ê°œ ì¤‘ ì—¬ìœ ë¶„ í™•ë³´)
    const total = files.length
    let processed = 0
    let errors = 0

    for (let i = 0; i < files.length; i += BATCH_SIZE) {
      if (this.shouldCancel) {
        throw new Error('Indexing cancelled')
      }

      const batch = files.slice(i, i + BATCH_SIZE)
      
      // ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ (ì—ëŸ¬ í—ˆìš©)
      const results = await Promise.allSettled(
        batch.map(file => this.processFile(file))
      )
      
      // ì—ëŸ¬ ì¹´ìš´íŠ¸
      for (const result of results) {
        if (result.status === 'rejected') {
          errors++
          // ì—ëŸ¬ ë¡œê·¸ëŠ” ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ
          if (errors <= 10) {
            console.warn('[IndexManager] File processing error:', result.reason?.message || result.reason)
          }
        }
      }
      
      processed += batch.length
      const elapsed = Date.now() - this.startTime
      const speed = processed / (elapsed / 1000)
      
      this.sendProgress({
        phase: 'files',
        current: processed,
        total,
        message: `íŒŒì¼ ì²˜ë¦¬ ì¤‘... ${processed}/${total} (${speed.toFixed(1)} files/sec)${errors > 0 ? ` ì—ëŸ¬:${errors}` : ''}`,
        speed
      })

      // ë°°ì¹˜ ê°„ ë”œë ˆì´ ìµœì†Œí™” (ìµœì í™”: 50â†’5)
      if (i + BATCH_SIZE < files.length) {
        await new Promise(r => setTimeout(r, 5))
      }
    }

    if (errors > 0) {
      console.log(`[IndexManager] Completed with ${errors} errors out of ${total} files`)
    }
  }

  /**
   * ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
   */
  private async processFile(file: FileChange): Promise<void> {
    try {
      // íŒŒì¼ ë‚´ìš© ì½ê¸°
      const content = await sshManager.readFile(this.serverId, file.path)
      
      // ìºì‹œì— ì €ì¥
      fileContentCache.set(file.path, content)
      
      // íŒŒì¼ íƒ€ì… íŒë‹¨
      const type = this.getFileType(file.name)
      
      // DBì— íŒŒì¼ ë“±ë¡
      const fileId = indexDb.insertFile({
        path: file.path,
        name: file.name,
        type,
        size: content.length,
        mtime: file.mtime
      })

      if (fileId < 0) return

      // ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìˆ˜ì •ëœ íŒŒì¼ì¸ ê²½ìš°)
      if (file.type === 'modified') {
        indexDb.deleteSymbolsByFile(fileId)
      }

      // íƒ€ì…ë³„ íŒŒì‹±
      switch (type) {
        case 'header':
          this.parseHeaderFile(fileId, content)
          break
        case 'dts':
          this.parseDtsFile(fileId, file.path, content)
          break
        case 'recipe':
        case 'config':
          this.parseBitbakeFile(fileId, content)
          break
      }

    } catch (err) {
      console.error(`[IndexManager] Failed to process ${file.path}:`, err)
    }
  }

  /**
   * íŒŒì¼ íƒ€ì… íŒë‹¨
   */
  private getFileType(name: string): FileRecord['type'] {
    if (name.endsWith('.bb') || name.endsWith('.bbappend') || name.endsWith('.inc')) return 'recipe'
    if (name.endsWith('.h')) return 'header'
    if (name.endsWith('.dts') || name.endsWith('.dtsi')) return 'dts'
    if (name.endsWith('.conf')) return 'config'
    if (name.endsWith('.c') || name.endsWith('.cpp')) return 'source'
    return 'other'
  }

  /**
   * í—¤ë” íŒŒì¼ íŒŒì‹± (#define ì¶”ì¶œ)
   */
  private parseHeaderFile(fileId: number, content: string): void {
    const symbols: Omit<SymbolRecord, 'id'>[] = []
    const lines = content.split('\n')

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i]
      
      // #define NAME VALUE
      const match = line.match(/^\s*#define\s+([A-Z_][A-Z0-9_]*)\s*(.*)$/)
      if (match) {
        const [, name, value] = match
        const cleanValue = value
          .replace(/\/\*.*?\*\//g, '')  // ì£¼ì„ ì œê±°
          .replace(/\/\/.*$/, '')        // ë¼ì¸ ì£¼ì„ ì œê±°
          .replace(/\\$/, '')            // ì¤„ ì—°ì† ì œê±°
          .trim()

        symbols.push({
          name,
          value: cleanValue,
          type: 'define',
          file_id: fileId,
          line: i + 1
        })
      }
    }

    if (symbols.length > 0) {
      indexDb.insertSymbols(symbols)
    }
  }

  /**
   * Device Tree íŒŒì¼ íŒŒì‹±
   */
  private parseDtsFile(fileId: number, filePath: string, content: string): void {
    const symbols: Omit<SymbolRecord, 'id'>[] = []
    const includes: Omit<IncludeRecord, 'id'>[] = []
    const nodes: Omit<DtNodeRecord, 'id'>[] = []
    const properties: Omit<DtPropertyRecord, 'id'>[] = []
    const gpioPins: Omit<GpioPinRecord, 'id'>[] = []

    const lines = content.split('\n')
    const nodeStack: { id: number; path: string }[] = []
    let currentNodeId = -1
    let currentPath = ''

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i]
      const trimmed = line.trim()
      const lineNum = i + 1

      // #include
      let match = trimmed.match(/^#include\s*[<"]([^>"]+)[>"]/)
      if (match) {
        includes.push({
          from_file_id: fileId,
          to_path: match[1],
          type: '#include',
          line: lineNum
        })
        continue
      }

      // /include/
      match = trimmed.match(/\/include\/\s*"([^"]+)"/)
      if (match) {
        includes.push({
          from_file_id: fileId,
          to_path: match[1],
          type: '#include',
          line: lineNum
        })
        continue
      }

      // ë…¸ë“œ ì‹œì‘: label: name@address {
      match = trimmed.match(/^(?:(\w+)\s*:\s*)?(\w+[-\w]*)(?:@([0-9a-fA-F]+))?\s*\{/)
      if (match) {
        const [, label, name, address] = match
        const nodePath = currentPath ? `${currentPath}/${name}` : `/${name}`
        
        nodes.push({
          file_id: fileId,
          path: nodePath,
          name,
          label,
          address,
          parent_id: currentNodeId > 0 ? currentNodeId : undefined,
          start_line: lineNum,
          end_line: lineNum  // ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
        })

        // ë¼ë²¨ì´ ìˆìœ¼ë©´ ì‹¬ë³¼ë¡œë„ ë“±ë¡
        if (label) {
          symbols.push({
            name: label,
            value: nodePath,
            type: 'label',
            file_id: fileId,
            line: lineNum
          })
        }

        nodeStack.push({ id: currentNodeId, path: currentPath })
        currentNodeId = nodes.length  // ì„ì‹œ ID
        currentPath = nodePath
        continue
      }

      // ë…¸ë“œ ì¢…ë£Œ
      if (trimmed === '};' || trimmed === '}') {
        if (nodeStack.length > 0) {
          const parent = nodeStack.pop()!
          currentNodeId = parent.id
          currentPath = parent.path
        }
        continue
      }

      // ì†ì„±: name = value;
      match = trimmed.match(/^([\w,#-]+)\s*(?:=\s*(.+?))?;$/)
      if (match && currentNodeId !== 0) {  // â˜… ìˆ˜ì •: ì˜¤ë²„ë¼ì´ë“œ ë…¸ë“œ(-1)ë„ í¬í•¨
        const [, propName, propValue] = match
        
        properties.push({
          node_id: currentNodeId,
          name: propName,
          value: propValue || '',
          line: lineNum
        })

        // â˜… ì†ì„± ê°’ì—ì„œ &label ì°¸ì¡° ì¶”ì¶œ (Find All References í•µì‹¬!)
        if (propValue) {
          const labelRefMatches = propValue.matchAll(/&(\w+)/g)
          for (const labelMatch of labelRefMatches) {
            const refLabel = labelMatch[1]
            // ì°¸ì¡°ë¥¼ ì‹¬ë³¼ë¡œ ì €ì¥ (type: 'label_ref')
            symbols.push({
              name: `&${refLabel}`,  // &uart0 í˜•íƒœë¡œ ì €ì¥
              value: refLabel,        // ì‹¤ì œ ë¼ë²¨ëª…
              type: 'label',          // ë¼ë²¨ íƒ€ì…
              file_id: fileId,
              line: lineNum
            })
          }
        }

        // GPIO ì†ì„± íŒŒì‹±
        if (propName.includes('gpio') || propName.includes('GPIO')) {
          const gpioMatches = (propValue || '').matchAll(/<\s*&(\w+)\s+(\d+)\s*(?:(\d+))?\s*>/g)
          for (const gpioMatch of gpioMatches) {
            const [, controller, pinStr, flags] = gpioMatch
            gpioPins.push({
              file_id: fileId,
              controller,
              pin: parseInt(pinStr),
              label: propName.replace(/-?gpio[s]?$/i, ''),
              function: propName,
              direction: flags === '0' ? 'out' : 'in',
              line: lineNum
            })
          }
        }
      }

      // â˜… ë…¸ë“œ ì°¸ì¡° (ì˜¤ë²„ë¼ì´ë“œ): &label { ... }
      match = trimmed.match(/^&(\w+)\s*\{/)
      if (match) {
        const refLabel = match[1]
        // ì˜¤ë²„ë¼ì´ë“œ ì°¸ì¡°ë„ ì‹¬ë³¼ë¡œ ì €ì¥
        symbols.push({
          name: `&${refLabel}`,
          value: refLabel,
          type: 'label',
          file_id: fileId,
          line: lineNum
        })
        
        // ìŠ¤íƒì— ì„ì‹œ ì¶”ê°€ (ë‚˜ì¤‘ì— }ì—ì„œ íŒ)
        nodeStack.push({ id: currentNodeId, path: currentPath })
        currentNodeId = -1  // ì˜¤ë²„ë¼ì´ë“œ ë…¸ë“œ
        currentPath = `&${refLabel}`
        continue
      }
    }

    // DBì— ì €ì¥
    if (symbols.length > 0) indexDb.insertSymbols(symbols)
    if (includes.length > 0) indexDb.insertIncludes(includes)
    if (nodes.length > 0) {
      const nodeIds = indexDb.insertDtNodes(nodes)
      // ì†ì„±ì˜ node_id ì—…ë°ì´íŠ¸
      const propsWithIds = properties.map((p) => ({
        ...p,
        node_id: nodeIds[p.node_id - 1] || p.node_id
      }))
      
      if (propsWithIds.length > 0) indexDb.insertDtProperties(propsWithIds)
    }
    if (gpioPins.length > 0) indexDb.insertGpioPins(gpioPins)
  }

  /**
   * BitBake íŒŒì¼ íŒŒì‹±
   */
  private parseBitbakeFile(fileId: number, content: string): void {
    const symbols: Omit<SymbolRecord, 'id'>[] = []
    const includes: Omit<IncludeRecord, 'id'>[] = []

    const lines = content.split('\n')

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i]
      const trimmed = line.trim()
      const lineNum = i + 1

      // require/include
      let match = trimmed.match(/^(require|include)\s+["']?([^"'\s]+)["']?/)
      if (match) {
        includes.push({
          from_file_id: fileId,
          to_path: match[2],
          type: match[1] as 'require' | 'include',
          line: lineNum
        })
        continue
      }

      // inherit
      match = trimmed.match(/^inherit\s+(.+)/)
      if (match) {
        const classes = match[1].split(/\s+/).filter(c => c && !c.startsWith('$'))
        for (const cls of classes) {
          includes.push({
            from_file_id: fileId,
            to_path: `classes/${cls}.bbclass`,
            type: 'inherit',
            line: lineNum
          })
        }
        continue
      }

      // ë³€ìˆ˜ í• ë‹¹: VAR = "value"
      match = trimmed.match(/^([A-Z_][A-Z0-9_]*)\s*(=|\?=|\?\?=|:=|\+=|\.=)\s*["']?(.*)["']?$/)
      if (match) {
        const [, name, op, value] = match
        symbols.push({
          name,
          value: value.replace(/["']$/, ''),
          type: 'variable',
          file_id: fileId,
          line: lineNum
        })
      }
    }

    if (symbols.length > 0) indexDb.insertSymbols(symbols)
    if (includes.length > 0) indexDb.insertIncludes(includes)
  }

  /**
   * ì¸ë±ì‹± ìƒíƒœ
   */
  getStatus(): { isIndexing: boolean; projectPath: string } {
    return {
      isIndexing: this.isIndexing,
      projectPath: this.projectPath
    }
  }

  /**
   * í†µê³„ ì¡°íšŒ
   */
  getStats(): ReturnType<typeof indexDb.getStats> & { lastIndexTime: string | null } {
    const stats = indexDb.getStats()
    const lastIndexTime = indexDb.getMetadata('last_index_time')
    return { ...stats, lastIndexTime }
  }

  // ============================================
  // ì„œë²„ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ (íŒ€ ê³µìœ ìš©)
  // ============================================

  /**
   * ì¸ë±ìŠ¤ë¥¼ ì„œë²„ì— ì €ì¥
   * ê²½ë¡œ: {projectPath}/.bsp-index/index.bspidx
   */
  async saveIndexToServer(serverId: string, projectPath: string): Promise<boolean> {
    try {
      if (!indexDb.isInitialized()) {
        console.error('[IndexManager] DB not initialized')
        return false
      }

      // WAL ì²´í¬í¬ì¸íŠ¸ (ëª¨ë“  ë³€ê²½ì‚¬í•­ì„ ë©”ì¸ íŒŒì¼ì— ê¸°ë¡)
      indexDb.checkpoint()

      const localDbPath = indexDb.getDbPath()
      const remoteDir = `${projectPath}/.bsp-index`
      const remoteDbPath = `${remoteDir}/index.bspidx`

      console.log(`[IndexManager] Saving index to server: ${remoteDbPath}`)

      // ì„œë²„ì— ë””ë ‰í† ë¦¬ ìƒì„±
      await sshManager.exec(serverId, `mkdir -p "${remoteDir}"`)

      // ë¡œì»¬ DB íŒŒì¼ ì½ê¸°
      const fs = await import('fs')
      const dbBuffer = fs.readFileSync(localDbPath)

      // ì„œë²„ì— ì—…ë¡œë“œ
      await sshManager.writeFile(serverId, remoteDbPath, dbBuffer)

      // ë©”íƒ€ë°ì´í„° ì €ì¥ (ë§ˆì§€ë§‰ ì €ì¥ ì‹œê°„)
      const metaPath = `${remoteDir}/meta.json`
      const meta = {
        lastSaved: new Date().toISOString(),
        savedBy: process.env.USERNAME || process.env.USER || 'unknown',
        stats: indexDb.getStats()
      }
      await sshManager.writeFile(serverId, metaPath, Buffer.from(JSON.stringify(meta, null, 2)))

      console.log('[IndexManager] Index saved to server successfully')
      return true

    } catch (err) {
      console.error('[IndexManager] Failed to save index to server:', err)
      return false
    }
  }

  /**
   * ì„œë²„ì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ
   * @returns true if loaded from server, false if not available
   */
  async loadIndexFromServer(serverId: string, projectPath: string): Promise<boolean> {
    try {
      const remoteDir = `${projectPath}/.bsp-index`
      const remoteDbPath = `${remoteDir}/index.bspidx`

      console.log(`[IndexManager] Checking server index: ${remoteDbPath}`)

      // ì„œë²„ì— ì¸ë±ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
      const exists = await sshManager.pathExists(serverId, remoteDbPath)
      if (!exists) {
        console.log('[IndexManager] No server index found')
        return false
      }

      // ë©”íƒ€ë°ì´í„° í™•ì¸
      const metaPath = `${remoteDir}/meta.json`
      let meta: { lastSaved: string; stats: any } | null = null
      try {
        const metaContent = await sshManager.readFile(serverId, metaPath)
        meta = JSON.parse(metaContent)
        console.log(`[IndexManager] Server index: saved at ${meta?.lastSaved}, ${meta?.stats?.files} files`)
      } catch {
        console.log('[IndexManager] No meta.json, loading anyway')
      }

      // ë¡œì»¬ DB ê²½ë¡œ í™•ë³´
      const { app } = await import('electron')
      const { join } = await import('path')
      const { existsSync, mkdirSync, writeFileSync } = await import('fs')

      const dataDir = join(app.getPath('userData'), 'indexes')
      if (!existsSync(dataDir)) {
        mkdirSync(dataDir, { recursive: true })
      }

      // í”„ë¡œì íŠ¸ í•´ì‹œë¡œ ë¡œì»¬ íŒŒì¼ëª… ìƒì„±
      const projectHash = this.hashPath(projectPath)
      const localDbPath = join(dataDir, `${projectHash}.bspidx`)

      // ì„œë²„ì—ì„œ DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ
      console.log('[IndexManager] Downloading index from server...')
      const dbContent = await sshManager.readFileBuffer(serverId, remoteDbPath)
      writeFileSync(localDbPath, dbContent)

      // DB ë‹¤ì‹œ ì—´ê¸°
      indexDb.close()
      indexDb.init(projectPath)

      console.log('[IndexManager] Index loaded from server successfully')
      return true

    } catch (err) {
      console.error('[IndexManager] Failed to load index from server:', err)
      return false
    }
  }

  /**
   * ì„œë²„ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
   */
  async getServerIndexMeta(serverId: string, projectPath: string): Promise<{
    exists: boolean
    lastSaved?: string
    savedBy?: string
    stats?: { files: number; symbols: number }
  }> {
    try {
      const metaPath = `${projectPath}/.bsp-index/meta.json`
      console.log(`[IndexManager] Checking server meta: ${metaPath}`)
      
      const exists = await sshManager.pathExists(serverId, metaPath)
      console.log(`[IndexManager] Server meta exists: ${exists}`)
      
      if (!exists) {
        return { exists: false }
      }

      const metaContent = await sshManager.readFile(serverId, metaPath)
      console.log(`[IndexManager] Server meta content: ${metaContent.slice(0, 200)}`)
      const meta = JSON.parse(metaContent)
      
      return {
        exists: true,
        lastSaved: meta.lastSaved,
        savedBy: meta.savedBy,
        stats: meta.stats
      }
    } catch (err) {
      console.error('[IndexManager] Failed to get server meta:', err)
      return { exists: false }
    }
  }

  /**
   * ê²½ë¡œ í•´ì‹œ (ë¡œì»¬ DB íŒŒì¼ëª…ìš©)
   */
  private hashPath(path: string): string {
    let hash = 0
    for (let i = 0; i < path.length; i++) {
      const char = path.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash
    }
    return Math.abs(hash).toString(16)
  }

  // ============================================
  // ğŸš€ ì„œë²„ ì¸¡ ê³ ì† ì¸ë±ì‹± (í•µí­íƒ„ê¸‰ ì„±ëŠ¥!)
  // ============================================

  /**
   * Python ì¸ë±ì„œ ìŠ¤í¬ë¦½íŠ¸ (ì„œë²„ì— ë°°í¬)
   * ë‚´ì¥: ì„œë²„ì—ì„œ ë¡œì»¬ I/Oë¡œ ì´ˆê³ ì† ì¸ë±ì‹±
   */
  private readonly INDEXER_SCRIPT = `#!/usr/bin/env python3
"""
BSP Indexer - ì„œë²„ ì¸¡ ê³ ì† ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸
Yocto/BSP í”„ë¡œì íŠ¸ë¥¼ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§ì ‘ íŒŒì‹±í•˜ì—¬ SQLite DB ìƒì„±
ì„±ëŠ¥: 10,000ê°œ íŒŒì¼ ê¸°ì¤€ ~30ì´ˆ (vs SSH ê°œë³„ ì½ê¸° ~10ë¶„)
"""
import os,re,sys,json,sqlite3,argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

FILE_TYPES = {'.bb':'recipe','.bbappend':'recipe','.inc':'recipe','.conf':'config','.h':'header','.dts':'dts','.dtsi':'dts'}
EXCLUDE_PATTERNS = ['*/tmp/work/*','*/.git/*','*/sstate-cache/*','*/downloads/*']

class BspIndexer:
    def __init__(self,project_path,output_path=None):
        self.project_path=Path(project_path).resolve()
        self.output_path=output_path or str(self.project_path/'.bsp-index'/'index.bspidx')
        self.conn=None
        self.stats={'files':0,'symbols':0,'includes':0,'dt_nodes':0,'gpio_pins':0}

    def run(self):
        print(f"[BSP Indexer] Project: {self.project_path}")
        print(f"[BSP Indexer] Output: {self.output_path}")
        start_time=datetime.now()
        os.makedirs(os.path.dirname(self.output_path),exist_ok=True)
        self.init_db()
        files=self.scan_files()
        print(f"[BSP Indexer] Found {len(files)} files to index")
        self.parse_files_parallel(files)
        self.save_metadata()
        elapsed=(datetime.now()-start_time).total_seconds()
        print(f"\\n[BSP Indexer] Completed in {elapsed:.1f}s")
        print(f"  Files: {self.stats['files']}, Symbols: {self.stats['symbols']}, DT Nodes: {self.stats['dt_nodes']}")
        self.conn.close()
        self.save_meta_json(elapsed)
        return self.output_path

    def init_db(self):
        if os.path.exists(self.output_path):os.remove(self.output_path)
        self.conn=sqlite3.connect(self.output_path)
        self.conn.execute("PRAGMA journal_mode=WAL")
        self.conn.execute("PRAGMA synchronous=NORMAL")
        self.conn.execute("PRAGMA cache_size=-64000")
        self.conn.executescript('''
CREATE TABLE IF NOT EXISTS files(id INTEGER PRIMARY KEY AUTOINCREMENT,path TEXT NOT NULL UNIQUE,name TEXT NOT NULL,type TEXT NOT NULL,size INTEGER,mtime INTEGER);
CREATE INDEX IF NOT EXISTS idx_files_path ON files(path);
CREATE INDEX IF NOT EXISTS idx_files_type ON files(type);
CREATE TABLE IF NOT EXISTS symbols(id INTEGER PRIMARY KEY AUTOINCREMENT,name TEXT NOT NULL,value TEXT,type TEXT NOT NULL,file_id INTEGER,line INTEGER NOT NULL);
CREATE INDEX IF NOT EXISTS idx_symbols_name ON symbols(name);
CREATE INDEX IF NOT EXISTS idx_symbols_file ON symbols(file_id);
CREATE TABLE IF NOT EXISTS includes(id INTEGER PRIMARY KEY AUTOINCREMENT,from_file_id INTEGER,to_path TEXT NOT NULL,type TEXT NOT NULL,line INTEGER NOT NULL);
CREATE INDEX IF NOT EXISTS idx_includes_from ON includes(from_file_id);
CREATE INDEX IF NOT EXISTS idx_includes_to ON includes(to_path);
CREATE TABLE IF NOT EXISTS dt_nodes(id INTEGER PRIMARY KEY AUTOINCREMENT,file_id INTEGER,path TEXT NOT NULL,name TEXT NOT NULL,label TEXT,address TEXT,parent_id INTEGER,start_line INTEGER NOT NULL,end_line INTEGER NOT NULL);
CREATE INDEX IF NOT EXISTS idx_dt_nodes_path ON dt_nodes(path);
CREATE INDEX IF NOT EXISTS idx_dt_nodes_label ON dt_nodes(label);
CREATE INDEX IF NOT EXISTS idx_dt_nodes_file ON dt_nodes(file_id);
CREATE TABLE IF NOT EXISTS dt_properties(id INTEGER PRIMARY KEY AUTOINCREMENT,node_id INTEGER,name TEXT NOT NULL,value TEXT,line INTEGER NOT NULL);
CREATE INDEX IF NOT EXISTS idx_dt_props_node ON dt_properties(node_id);
CREATE INDEX IF NOT EXISTS idx_dt_props_name ON dt_properties(name);
CREATE TABLE IF NOT EXISTS gpio_pins(id INTEGER PRIMARY KEY AUTOINCREMENT,file_id INTEGER,controller TEXT NOT NULL,pin INTEGER NOT NULL,label TEXT,function TEXT,direction TEXT);
CREATE TABLE IF NOT EXISTS metadata(key TEXT PRIMARY KEY,value TEXT);
CREATE VIRTUAL TABLE IF NOT EXISTS symbols_fts USING fts5(name,value,content='symbols',content_rowid='id');
CREATE TRIGGER IF NOT EXISTS symbols_ai AFTER INSERT ON symbols BEGIN INSERT INTO symbols_fts(rowid,name,value) VALUES (new.id,new.name,new.value); END;
''')
        self.conn.commit()

    def scan_files(self):
        import fnmatch
        files=[]
        for root,dirs,filenames in os.walk(self.project_path):
            rel_root=os.path.relpath(root,self.project_path)
            skip=any(fnmatch.fnmatch(rel_root,p) or fnmatch.fnmatch('/'+rel_root,p) for p in EXCLUDE_PATTERNS)
            if skip:dirs.clear();continue
            for filename in filenames:
                ext=os.path.splitext(filename)[1].lower()
                if ext in FILE_TYPES:files.append({'path':os.path.join(root,filename),'name':filename,'type':FILE_TYPES[ext],'ext':ext})
        return files

    def parse_files_parallel(self,files,max_workers=8):
        total=len(files);processed=0;batch_size=100
        for i in range(0,len(files),batch_size):
            batch=files[i:i+batch_size];results=[]
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures={executor.submit(self.parse_file,f):f for f in batch}
                for future in as_completed(futures):
                    try:
                        result=future.result()
                        if result:results.append(result)
                    except:pass
            self.insert_batch(results)
            processed+=len(batch)
            print(f"\\r[BSP Indexer] Progress: {processed}/{total} ({processed/total*100:.1f}%)",end='',flush=True)
        print()

    def parse_file(self,file_info):
        try:
            filepath=file_info['path'];stat=os.stat(filepath)
            with open(filepath,'r',encoding='utf-8',errors='ignore') as f:content=f.read()
            result={'file':{'path':os.path.relpath(filepath,self.project_path),'name':file_info['name'],'type':file_info['type'],'size':stat.st_size,'mtime':int(stat.st_mtime)},'symbols':[],'includes':[],'dt_nodes':[],'dt_properties':[]}
            if file_info['type'] in ('recipe','config'):self._parse_bitbake(content,result)
            elif file_info['type']=='dts':self._parse_dts(content,result)
            elif file_info['type']=='header':self._parse_header(content,result)
            return result
        except:return None

    def _parse_bitbake(self,content,result):
        for i,line in enumerate(content.split('\\n')):
            line_num=i+1;stripped=line.strip()
            match=re.match(r'^([A-Za-z_][A-Za-z0-9_-]*)\\s*(\\??\\+?=|:=|\\.=)\\s*["\\'\\']?([^"\\'\\']*)' ,stripped)
            if match:result['symbols'].append({'name':match.group(1),'value':match.group(3)[:200],'type':'variable','line':line_num})
            match=re.match(r'^(require|include)\\s+["\\'\\'"]?([^"\\'\\'\\s]+)',stripped)
            if match:result['includes'].append({'to_path':match.group(2),'type':match.group(1),'line':line_num})
            match=re.match(r'^inherit\\s+(.+)',stripped)
            if match:
                for cls in match.group(1).split():result['includes'].append({'to_path':f"classes/{cls}.bbclass",'type':'inherit','line':line_num})

    def _parse_dts(self,content,result):
        node_stack=[];current_path=''
        for i,line in enumerate(content.split('\\n')):
            line_num=i+1;stripped=line.strip()
            match=re.match(r'#include\\s*[<"]([^>"]+)[>"]',stripped)
            if match:result['includes'].append({'to_path':match.group(1),'type':'#include','line':line_num});continue
            match=re.match(r'^(?:(\\w+)\\s*:\\s*)?(\\S+?)(?:@([0-9a-fA-F]+))?\\s*\\{',stripped)
            if match:
                label,name,address=match.group(1),match.group(2),match.group(3)
                new_path=name if name.startswith('&') else (f"{current_path}/{name}" if current_path else f"/{name}")
                node_stack.append((current_path,line_num));current_path=new_path
                result['dt_nodes'].append({'path':new_path,'name':name,'label':label,'address':address,'start_line':line_num,'end_line':line_num})
                if label:result['symbols'].append({'name':label,'value':new_path,'type':'label','line':line_num})
                continue
            if stripped in ('};','}'):
                if node_stack:
                    parent_path,start=node_stack.pop()
                    for node in reversed(result['dt_nodes']):
                        if node['path']==current_path:node['end_line']=line_num;break
                    current_path=parent_path
                continue
            match=re.match(r'^([\\w,#-]+)\\s*(?:=\\s*(.+?))?;$',stripped)
            if match and current_path:
                prop_name,prop_value=match.group(1),match.group(2) or ''
                result['dt_properties'].append({'node_path':current_path,'name':prop_name,'value':prop_value[:500],'line':line_num})
                for ref_match in re.finditer(r'&(\\w+)',prop_value):result['symbols'].append({'name':f"&{ref_match.group(1)}",'value':ref_match.group(1),'type':'label_ref','line':line_num})

    def _parse_header(self,content,result):
        for i,line in enumerate(content.split('\\n')):
            line_num=i+1;stripped=line.strip()
            match=re.match(r'^#define\\s+([A-Za-z_][A-Za-z0-9_]*)\\s*(.*)',stripped)
            if match:result['symbols'].append({'name':match.group(1),'value':match.group(2)[:200],'type':'define','line':line_num})
            match=re.match(r'^#include\\s*[<"]([^>"]+)[>"]',stripped)
            if match:result['includes'].append({'to_path':match.group(1),'type':'#include','line':line_num})

    def insert_batch(self,results):
        cursor=self.conn.cursor()
        for result in results:
            if not result:continue
            cursor.execute("INSERT OR REPLACE INTO files(path,name,type,size,mtime) VALUES(?,?,?,?,?)",(result['file']['path'],result['file']['name'],result['file']['type'],result['file']['size'],result['file']['mtime']))
            file_id=cursor.lastrowid;self.stats['files']+=1
            for sym in result['symbols']:cursor.execute("INSERT INTO symbols(name,value,type,file_id,line) VALUES(?,?,?,?,?)",(sym['name'],sym.get('value'),sym['type'],file_id,sym['line']));self.stats['symbols']+=1
            for inc in result['includes']:cursor.execute("INSERT INTO includes(from_file_id,to_path,type,line) VALUES(?,?,?,?)",(file_id,inc['to_path'],inc['type'],inc['line']));self.stats['includes']+=1
            node_id_map={}
            for node in result['dt_nodes']:cursor.execute("INSERT INTO dt_nodes(file_id,path,name,label,address,parent_id,start_line,end_line) VALUES(?,?,?,?,?,?,?,?)",(file_id,node['path'],node['name'],node.get('label'),node.get('address'),None,node['start_line'],node['end_line']));node_id_map[node['path']]=cursor.lastrowid;self.stats['dt_nodes']+=1
            for prop in result['dt_properties']:
                node_id=node_id_map.get(prop['node_path'])
                if node_id:cursor.execute("INSERT INTO dt_properties(node_id,name,value,line) VALUES(?,?,?,?)",(node_id,prop['name'],prop.get('value'),prop['line']))
        self.conn.commit()

    def save_metadata(self):
        cursor=self.conn.cursor()
        cursor.execute("INSERT OR REPLACE INTO metadata VALUES(?,?)",('last_index_time',str(int(datetime.now().timestamp()*1000))))
        cursor.execute("INSERT OR REPLACE INTO metadata VALUES(?,?)",('project_path',str(self.project_path)))
        cursor.execute("INSERT OR REPLACE INTO metadata VALUES(?,?)",('indexer_version','2.0-server'))
        self.conn.commit()

    def save_meta_json(self,elapsed):
        meta={'lastSaved':datetime.now().isoformat(),'savedBy':os.environ.get('USER',os.environ.get('USERNAME','unknown')),'indexerVersion':'2.0-server','elapsed':round(elapsed,1),'stats':self.stats}
        meta_path=os.path.join(os.path.dirname(self.output_path),'meta.json')
        with open(meta_path,'w') as f:json.dump(meta,f,indent=2)

def main():
    parser=argparse.ArgumentParser(description='BSP Indexer - ì„œë²„ ì¸¡ ê³ ì† ì¸ë±ì‹±')
    parser.add_argument('project_path',help='Yocto/BSP í”„ë¡œì íŠ¸ ê²½ë¡œ')
    parser.add_argument('--output','-o',help='ì¶œë ¥ DB ê²½ë¡œ')
    args=parser.parse_args()
    indexer=BspIndexer(args.project_path,args.output)
    output_path=indexer.run()
    print(f"\\nâœ… Index saved: {output_path}")
    return 0

if __name__=='__main__':sys.exit(main())
`;

  /**
   * ğŸš€ ì„œë²„ ì¸¡ ê³ ì† ì¸ë±ì‹± ì‹¤í–‰
   * Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì„œë²„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ì´ˆê³ ì† ì¸ë±ì‹±
   * 
   * @returns Promise<boolean> ì„±ê³µ ì—¬ë¶€
   */
  async startServerSideIndexing(projectPath: string, serverId: string): Promise<boolean> {
    if (this.isIndexing) {
      console.log('[IndexManager] Already indexing')
      return false
    }

    this.isIndexing = true
    this.shouldCancel = false
    this.projectPath = projectPath
    this.serverId = serverId
    this.startTime = Date.now()

    try {
      this.sendProgress({ phase: 'init', current: 0, total: 0, message: 'ì„œë²„ ì¸¡ ì¸ë±ì‹± ì¤€ë¹„ ì¤‘...' })

      // 1. Python ìŠ¤í¬ë¦½íŠ¸ ì„œë²„ì— ë°°í¬
      const scriptPath = `${projectPath}/.bsp-index/indexer.py`
      console.log(`[IndexManager] Deploying indexer script to: ${scriptPath}`)

      await sshManager.exec(serverId, `mkdir -p "${projectPath}/.bsp-index"`)
      await sshManager.writeFile(serverId, scriptPath, Buffer.from(this.INDEXER_SCRIPT))
      await sshManager.exec(serverId, `chmod +x "${scriptPath}"`)

      // 2. Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
      this.sendProgress({ phase: 'files', current: 0, total: 0, message: 'ğŸš€ ì„œë²„ì—ì„œ ì¸ë±ì‹± ì‹¤í–‰ ì¤‘... (ì•½ 30ì´ˆ)' })
      
      console.log(`[IndexManager] Running server-side indexer...`)
      const result = await sshManager.exec(serverId, `cd "${projectPath}" && python3 "${scriptPath}" "${projectPath}"`, {
        timeout: 30 * 60 * 1000  // 30ë¶„ íƒ€ì„ì•„ì›ƒ (ëŒ€ìš©ëŸ‰ í”„ë¡œì íŠ¸ìš©)
      })

      if (result.code !== 0) {
        throw new Error(`Server indexer failed: ${result.stderr}`)
      }

      console.log(`[IndexManager] Server indexer output:\n${result.stdout}`)

      // 3. ìƒì„±ëœ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ
      this.sendProgress({ phase: 'files', current: 50, total: 100, message: 'ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ ì¤‘...' })
      
      const remoteDbPath = `${projectPath}/.bsp-index/index.bspidx`
      const loaded = await this.loadIndexFromServer(serverId, projectPath)

      if (!loaded) {
        throw new Error('Failed to load index from server')
      }

      // 4. ì™„ë£Œ
      const stats = indexDb.getStats()
      const elapsed = Date.now() - this.startTime

      this.sendProgress({
        phase: 'done',
        current: stats.files,
        total: stats.files,
        message: `ğŸ‰ ì™„ë£Œ! ${stats.files}íŒŒì¼, ${stats.symbols}ì‹¬ë³¼ (${(elapsed / 1000).toFixed(1)}ì´ˆ)`,
        speed: stats.files / (elapsed / 1000)
      })

      console.log(`[IndexManager] Server-side indexing completed in ${elapsed}ms:`, stats)

      this.isIndexing = false
      return true

    } catch (err: any) {
      console.error('[IndexManager] Server-side indexing error:', err)
      this.sendProgress({ phase: 'error', current: 0, total: 0, message: `ì—ëŸ¬: ${err.message}` })
      this.isIndexing = false
      return false
    }
  }

  /**
   * Python ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
   */
  async checkPythonAvailable(serverId: string): Promise<{ available: boolean; version?: string }> {
    try {
      const result = await sshManager.exec(serverId, 'python3 --version')
      if (result.code === 0) {
        return { available: true, version: result.stdout.trim() }
      }
      return { available: false }
    } catch {
      return { available: false }
    }
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const indexManager = new IndexManager()
